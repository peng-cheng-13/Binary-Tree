#!/usr/bin/env python
#Author: Peng Cheng
#Email: peng_cheng_13@163.com

from __future__ import division

import getpass
import sys
import math
import os
import re
import subprocess
import ConfigParser

from Pegasus.DAX3 import *


run_id = sys.argv[1]
base_dir = sys.argv[2]
tool_dir = sys.argv[3]
depth = int(sys.argv[4])
binary_file = []
processed_file = []
reduce_file = []
parent_job = []
child_job = []

# Create a DAX
dax = ADAG("binary-tree")

# Add executables to the DAX-level replica catalog
e_Createfile = Executable(name="createfile", arch="x86_64", installed=True)
e_Createfile.addPFN(PFN(base_dir + "/" + tool_dir + "/createfile" , "condorpool"))
dax.addExecutable(e_Createfile)

e_Binary = Executable(name="binary", arch="x86_64", installed=True)
e_Binary.addPFN(PFN(base_dir + "/" + tool_dir + "/binary" , "condorpool"))
dax.addExecutable(e_Binary)

e_Process = Executable(name="process", arch="x86_64", installed=True)
e_Process.addPFN(PFN(base_dir + "/" + tool_dir + "/process" , "condorpool"))
dax.addExecutable(e_Process)

e_Reduce = Executable(name="reduce", arch="x86_64", installed=True)
e_Reduce.addPFN(PFN(base_dir + "/" + tool_dir + "/reduce" , "condorpool"))
dax.addExecutable(e_Reduce)


# Add the first createfile job
j_createfile = Job(e_Createfile)
c_arg = "level-1-file-1"
binary_file.append(c_arg)
c_out = File(c_arg)
j_createfile.addArguments(c_arg,"${DATA_SIZE}","${INTER_DIR}/scratch/${RUN_ID}")
j_createfile.uses(c_out, link=Link.OUTPUT, transfer=False)
dax.addJob(j_createfile)

# Add binary jobs with depth d
for i in range(1,depth):
    job_id = 1 
    # Update parent jobs
    if i==1:
	parent_job.append(j_createfile)
    else:
	parent_job = child_job
	child_job = []

    for in_name in binary_file:
	prefix = "level-"+ str(i)
	# only need files in cuurent level
	if not prefix in in_name:
	    continue
	in_file = File(in_name)
	# Each input file is is consumed by two children
	# child 1
	j_binary1 = Job(e_Binary)
	b_outputname1 = "level-"+ str(i+1) + "-file-"+str(job_id)
	b_outputfile1 = File(b_outputname1)
	tmppath = "${INTER_DIR}/scratch/${RUN_ID}/"+ in_name
	binary_file.append(b_outputname1)
	job_id = job_id + 1
	j_binary1.addArguments(in_name,b_outputname1,"${DATA_SIZE}","${INTER_DIR}/scratch/${RUN_ID}")
	# j_binary1.profile(Namespace.CONDOR , "input", tmppath)
	j_binary1.uses(in_file,link=Link.INPUT)
	j_binary1.uses(b_outputname1, link=Link.OUTPUT, transfer=False)
	dax.addJob(j_binary1)
	# child 2
	j_binary2 = Job(e_Binary)
	b_outputname2 = "level-"+ str(i+1) + "-file-"+str(job_id)
        b_outputfile2 = File(b_outputname1)
        tmppath = "${INTER_DIR}/scratch/${RUN_ID}/"+ in_name
        binary_file.append(b_outputname2)
        job_id = job_id + 1
        j_binary2.addArguments(in_name,b_outputname2,"${DATA_SIZE}","${INTER_DIR}/scratch/${RUN_ID}")
        # j_binary2.profile(Namespace.CONDOR , "input", tmppath)
        j_binary2.uses(in_file,link=Link.INPUT)
        j_binary2.uses(b_outputname2, link=Link.OUTPUT, transfer=False)
	dax.addJob(j_binary2)
	# Update child jobs
	child_job.append(j_binary1)
	child_job.append(j_binary2)

    # Add dependencies fro binary jobs
    for i in parent_job:
	for j in child_job:
	    dax.depends(parent=i, child=j)


# Add process jobs
processed_file =  binary_file
for l in range(1,depth):
    p_id = 1 
    # Initiate parent jobs from the lastest level binary jobs
    parent_job = child_job
    child_job = []
    tmp_file = []
    for in_name in processed_file:
        if l == 1 :
            prefix = "level-" + str(depth)
            # only need files in cuurent level
            if not prefix in in_name:
	        continue
        in_file = File(in_name)
        j_process = Job(e_Process)
        p_outputname = "level-"+ str(l) + "-processd-file-" + str(p_id)
        p_outputfile = File(p_outputname)
        tmppath = "${INTER_DIR}/scratch/${RUN_ID}/" + in_name
        tmp_file.append(p_outputname)
	if l == (depth-1):
	    reduce_file.append(p_outputname)
        p_id = p_id + 1
        j_process.addArguments(in_name,p_outputname,"${INTER_DIR}/scratch/${RUN_ID}")
        # j_process.profile(Namespace.CONDOR , "input", tmppath)
        j_process.uses(in_file,link=Link.INPUT)
        j_process.uses(p_outputfile, link=Link.OUTPUT, transfer=False)
        dax.addJob(j_process)
        #Update dependences for process jobs
        child_job.append(j_process)

    # update processed file info 
    processed_file = tmp_file

    # Add dependencies for process jobs
    for i in parent_job:
        for j in child_job:
	    dax.depends(parent=i, child=j)


# Add reduce job
for  i in range(1,depth):
    job_id = 1
    tmp_file = []
    parent_job = child_job
    child_job = []
    for index in range( int(len(reduce_file) / 2) ):
	# Each reduce job merge two processed file into one file
	j_reduce = Job(e_Reduce)
	in_name1 = reduce_file[index*2]
	in_name2 = reduce_file[index*2 + 1]
	# only need files in cuurent level
	#if not prefix in in_name:
	#    continue
	in_file1 = File(in_name1)
	in_file2 = File(in_name2)
	r_outputname = "level-"+str(i)+"-mergedfile-"+str(job_id)
	tmp_file.append(r_outputname)
	tmppath = "${INTER_DIR}/scratch/${RUN_ID}/" + in_name1
	out_file = File(r_outputname)
	job_id = job_id + 1
	j_reduce.addArguments(in_file1, in_file2, out_file,"${INTER_DIR}/scratch/${RUN_ID}")
	# j_reduce.profile(Namespace.CONDOR , "input", tmppath)
	j_reduce.uses(in_file1, link=Link.INPUT)
	j_reduce.uses(in_file2, link=Link.INPUT)
	if i == (depth-1):
	    j_reduce.uses(out_file, link=Link.OUTPUT, transfer=True)
	else:
	    j_reduce.uses(out_file, link=Link.OUTPUT, transfer=False)

	dax.addJob(j_reduce)
	# Update dependences for reduce jobs
        child_job.append(j_reduce)
    reduce_file = tmp_file
    # Add dependencies fro reduce jobs
    for i in parent_job:
        for j in child_job:
            dax.depends(parent=i, child=j)

# Write the DAX to a file
f = open("dag.xml","w")
dax.writeXML(f)
f.close()
